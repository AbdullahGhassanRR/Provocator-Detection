{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a0eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0124627f-7197-4870-ae7f-537ac828627b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Abdullah Ghassan\\\\env - semester 5\\\\softeng'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aff71b2e-c9c7-42ac-88a6-a5943807ea3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mohon ulama lurus dan k212 memberi hujjah part...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>tidak kecewa</td>\n",
       "      <td>positive</td>\n",
       "      <td>tidak kecewa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>enak rasa masakan nya apalagi kepiting yang me...</td>\n",
       "      <td>positive</td>\n",
       "      <td>enak rasa masakan nya apalagi kepiting yang me...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>hormati partai-partai yang telah berkoalisi</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hormati partaipartai yang telah berkoalisi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>pagi pagi di tol pasteur sudah macet parah , b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>pagi pagi di tol pasteur sudah macet parah , b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>meskipun sering belanja ke yogya di riau junct...</td>\n",
       "      <td>positive</td>\n",
       "      <td>meskipun sering belanja ke yogya di riau junct...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "0      warung ini dimiliki oleh pengusaha pabrik tahu...  positive   \n",
       "1      mohon ulama lurus dan k212 mmbri hujjah partai...   neutral   \n",
       "2      lokasi strategis di jalan sumatera bandung . t...  positive   \n",
       "3      betapa bahagia nya diri ini saat unboxing pake...  positive   \n",
       "4      duh . jadi mahasiswa jangan sombong dong . kas...  negative   \n",
       "...                                                  ...       ...   \n",
       "10995                                       tidak kecewa  positive   \n",
       "10996  enak rasa masakan nya apalagi kepiting yang me...  positive   \n",
       "10997        hormati partai-partai yang telah berkoalisi   neutral   \n",
       "10998  pagi pagi di tol pasteur sudah macet parah , b...  negative   \n",
       "10999  meskipun sering belanja ke yogya di riau junct...  positive   \n",
       "\n",
       "                                              clean_text  label  \n",
       "0      warung ini dimiliki oleh pengusaha pabrik tahu...      2  \n",
       "1      mohon ulama lurus dan k212 memberi hujjah part...      0  \n",
       "2      lokasi strategis di jalan sumatera bandung . t...      2  \n",
       "3      betapa bahagia nya diri ini saat unboxing pake...      2  \n",
       "4      duh . jadi mahasiswa jangan sombong dong . kas...      1  \n",
       "...                                                  ...    ...  \n",
       "10995                                       tidak kecewa      2  \n",
       "10996  enak rasa masakan nya apalagi kepiting yang me...      2  \n",
       "10997         hormati partaipartai yang telah berkoalisi      0  \n",
       "10998  pagi pagi di tol pasteur sudah macet parah , b...      1  \n",
       "10999  meskipun sering belanja ke yogya di riau junct...      2  \n",
       "\n",
       "[11000 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7020b6c8-95e8-4162-9ae4-648772135b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[0,'clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45db9938-5278-40b1-888c-83e550cd13f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/alvinhanafie/dataset-for-indonesian-sentiment-analysis?dataset_version_number=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 860k/860k [00:01<00:00, 615kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: C:\\Users\\Abdullah Ghassan\\.cache\\kagglehub\\datasets\\alvinhanafie\\dataset-for-indonesian-sentiment-analysis\\versions\\4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"alvinhanafie/dataset-for-indonesian-sentiment-analysis\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc649c80-fd11-4887-be66-3a49f3c34a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_preprocess_ori.tsv', sep = '\\t')\n",
    "test = pd.read_csv('valid_preprocess.tsv', sep = '\\t')\n",
    "val = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b6d6d82-07c2-455d-a213-05f4e68d4cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>tidak kecewa</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>enak rasa masakan nya apalagi kepiting yang me...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>hormati partai-partai yang telah berkoalisi</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>pagi pagi di tol pasteur sudah macet parah , b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>meskipun sering belanja ke yogya di riau junct...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0      warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
       "1      mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
       "2      lokasi strategis di jalan sumatera bandung . t...  positive\n",
       "3      betapa bahagia nya diri ini saat unboxing pake...  positive\n",
       "4      duh . jadi mahasiswa jangan sombong dong . kas...  negative\n",
       "...                                                  ...       ...\n",
       "10995                                       tidak kecewa  positive\n",
       "10996  enak rasa masakan nya apalagi kepiting yang me...  positive\n",
       "10997        hormati partai-partai yang telah berkoalisi   neutral\n",
       "10998  pagi pagi di tol pasteur sudah macet parah , b...  negative\n",
       "10999  meskipun sering belanja ke yogya di riau junct...  positive\n",
       "\n",
       "[11000 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad974c11-9975-4d3b-8403-16510ba25eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meski masa kampanye sudah selesai , bukan bera...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tidak enak</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>restoran ini menawarkan makanan sunda . kami m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lokasi di alun alun masakan padang ini cukup t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>betapa bejad kader gerindra yang anggota dprd ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>film tncfu , tidak cocok untuk penonton yang t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>indihome ini mahal loh bayar nya . hanya , pen...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>be de gea , cowok cupu yang takut dengan pacar...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>valen yang sangat tidak berkualitas . konentat...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>restoran ini menjadi tempat pilihan saya berbu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment\n",
       "0     meski masa kampanye sudah selesai , bukan bera...   neutral\n",
       "1                                            tidak enak  negative\n",
       "2     restoran ini menawarkan makanan sunda . kami m...  positive\n",
       "3     lokasi di alun alun masakan padang ini cukup t...  positive\n",
       "4     betapa bejad kader gerindra yang anggota dprd ...  negative\n",
       "...                                                 ...       ...\n",
       "1255  film tncfu , tidak cocok untuk penonton yang t...  negative\n",
       "1256  indihome ini mahal loh bayar nya . hanya , pen...  negative\n",
       "1257  be de gea , cowok cupu yang takut dengan pacar...  negative\n",
       "1258  valen yang sangat tidak berkualitas . konentat...  negative\n",
       "1259  restoran ini menjadi tempat pilihan saya berbu...  positive\n",
       "\n",
       "[1260 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a9dbd48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>makanan beragam , harga makanan di food stall ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pakai kartu kredit bca tidak untung malah rugi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tempat unik , bagus buat foto , makanan enak ,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>saya bersama keluarga baru saja menikmati peng...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bersyukur</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>simcard indosat inaktiv gara-gara lupa isi pul...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sifat iri sering muncul pada orang orang yang ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sekadar menceritakan pengalaman saya pesan ste...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pengalaman bersama indosat hari ini , semoga t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>anak sekarang sulit untuk dinasehati</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hanya mengatasi masalah kayak gini saja anies ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>setiap bumn dibentuk dengan uu bukan dibentuk ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sehabis puas bermain di trans studio bandung ,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rasa bakso cuanki dan batagor cukup . selalu r...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sgwr 2018 beri dampak positif terhadap hobi an...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>berada di lembang , pemandangan indah , udara ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>berbuka puasa di sini pada minggu lepas , pak ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hahaha suka banget nonton kartun indonesia kel...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bagi teman-teman yang sedang berkunjung ke ban...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>di restoran ini , saya dan keluarga makan mala...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>alhamdulillah hari ini tidak ke jalan bugel ag...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>respon agak lama , sama sayang nya rem nya buk...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lokasi dengan pemandangan alam yang masih natu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pdip sebut ridwan kamil menang karena berbaju ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>malu - maluin nih oknum yang tidak bertanggung...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>makan siang di sini asyik juga . nuansa bali w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tempat steak di bandung sejak lama dari zaman ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>08:30 : kedatangan presiden ri dan rombongan d...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>makanan tradisional yang selalu rasanya stabil...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sampai saat ini saya rasa batagor ini masih ya...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>aroma khas yang ada jika makan di atas daun pi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>akhir tahun lalu saya dan keluarga berjalan ke...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>risma menjatuhkan pilihan nya pada gus ipul - ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>pengalaman yang cukup seru , makan tanpa alas ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>makanan nya enak , suasana nyaman . harga bisa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>kampung daun adalah tempat makan yang menyatuk...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>waktu itu ke sini pesan nasi goreng dan kentan...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>banyak sekali tempat makan sunda yang bertebar...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>hai tolong bantu saya tidak bisa input tidak r...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>foods : menu baru cheeseburger nya oke banget ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>dusun bambu ini tergolong baru di bandung . te...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tidak memuaskan</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>suruh ngaca pemain nya ! dasar oneng ! ngomong...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>kafe ini menyajikan sensasi makan dalam gelap ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>sesuai dengan nama nya mi awie , kedai makan i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment\n",
       "0   warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
       "1   mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
       "2   lokasi strategis di jalan sumatera bandung . t...  positive\n",
       "3   betapa bahagia nya diri ini saat unboxing pake...  positive\n",
       "4   duh . jadi mahasiswa jangan sombong dong . kas...  negative\n",
       "5   makanan beragam , harga makanan di food stall ...  positive\n",
       "6   pakai kartu kredit bca tidak untung malah rugi...  negative\n",
       "7   tempat unik , bagus buat foto , makanan enak ,...  positive\n",
       "8   saya bersama keluarga baru saja menikmati peng...  positive\n",
       "9                                           bersyukur  positive\n",
       "10  simcard indosat inaktiv gara-gara lupa isi pul...  negative\n",
       "11  sifat iri sering muncul pada orang orang yang ...  negative\n",
       "12  sekadar menceritakan pengalaman saya pesan ste...  positive\n",
       "13  pengalaman bersama indosat hari ini , semoga t...  negative\n",
       "14               anak sekarang sulit untuk dinasehati  negative\n",
       "15  hanya mengatasi masalah kayak gini saja anies ...  negative\n",
       "16  setiap bumn dibentuk dengan uu bukan dibentuk ...   neutral\n",
       "17  sehabis puas bermain di trans studio bandung ,...  positive\n",
       "18  rasa bakso cuanki dan batagor cukup . selalu r...  positive\n",
       "19  sgwr 2018 beri dampak positif terhadap hobi an...  positive\n",
       "20  berada di lembang , pemandangan indah , udara ...  positive\n",
       "21  berbuka puasa di sini pada minggu lepas , pak ...  positive\n",
       "22  hahaha suka banget nonton kartun indonesia kel...  positive\n",
       "23  bagi teman-teman yang sedang berkunjung ke ban...  positive\n",
       "24  di restoran ini , saya dan keluarga makan mala...  positive\n",
       "25  alhamdulillah hari ini tidak ke jalan bugel ag...  negative\n",
       "26  respon agak lama , sama sayang nya rem nya buk...  negative\n",
       "27  lokasi dengan pemandangan alam yang masih natu...  positive\n",
       "28  pdip sebut ridwan kamil menang karena berbaju ...   neutral\n",
       "29  malu - maluin nih oknum yang tidak bertanggung...  negative\n",
       "30  makan siang di sini asyik juga . nuansa bali w...  negative\n",
       "31  tempat steak di bandung sejak lama dari zaman ...  positive\n",
       "32  08:30 : kedatangan presiden ri dan rombongan d...   neutral\n",
       "33  makanan tradisional yang selalu rasanya stabil...  positive\n",
       "34  sampai saat ini saya rasa batagor ini masih ya...  positive\n",
       "35  aroma khas yang ada jika makan di atas daun pi...  positive\n",
       "36  akhir tahun lalu saya dan keluarga berjalan ke...  positive\n",
       "37  risma menjatuhkan pilihan nya pada gus ipul - ...   neutral\n",
       "38  pengalaman yang cukup seru , makan tanpa alas ...  positive\n",
       "39  makanan nya enak , suasana nyaman . harga bisa...  positive\n",
       "40  kampung daun adalah tempat makan yang menyatuk...  positive\n",
       "41  waktu itu ke sini pesan nasi goreng dan kentan...  positive\n",
       "42  banyak sekali tempat makan sunda yang bertebar...  positive\n",
       "43  hai tolong bantu saya tidak bisa input tidak r...   neutral\n",
       "44  foods : menu baru cheeseburger nya oke banget ...  positive\n",
       "45  dusun bambu ini tergolong baru di bandung . te...  positive\n",
       "46                                    tidak memuaskan  negative\n",
       "47  suruh ngaca pemain nya ! dasar oneng ! ngomong...  negative\n",
       "48  kafe ini menyajikan sensasi makan dalam gelap ...  negative\n",
       "49  sesuai dengan nama nya mi awie , kedai makan i...  positive"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ef101",
   "metadata": {},
   "source": [
    "# **Model 1: modernBERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136a28c",
   "metadata": {},
   "source": [
    "## Preprocessing for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd170a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def specialized_cleaning(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Hapus bagian \"Nb: ...\"\n",
    "    text = re.sub(r'nb:.*', '', text)\n",
    "    \n",
    "    # Normalisasi karakter berulang (\"sakiitttt\" -> \"sakit\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "    # Ganti baris baru dengan spasi\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Kamus Slang\n",
    "    slang_dict = {\n",
    "        r'\\buu\\b': 'undang undang',\n",
    "        r'\\bga\\b': 'tidak',\n",
    "        r'\\bgak\\b': 'tidak',\n",
    "        r'\\bgw\\b': 'aku', \n",
    "        r'\\bkrn\\b': 'karena',\n",
    "        r'\\bkalo\\b': 'kalau',\n",
    "        r'\\bkl\\b': 'kalau', \n",
    "        r'\\btdk\\b': 'tidak',\n",
    "        r'\\bsy\\b': 'saya',\n",
    "        r'\\bpie\\b': 'bagaimana', \n",
    "        r'\\bbgt\\b': 'banget',\n",
    "        r'\\bdgn\\b': 'dengan',\n",
    "        r'\\biki\\b': 'ini', \n",
    "        r'\\bsdh\\b': 'sudah', \n",
    "        r'\\baja\\b': 'saja',\n",
    "        r'\\btrus\\b': 'terus', \n",
    "        r'\\bsm\\b': 'sama',\n",
    "        r'\\bdtg\\b': 'datang',\n",
    "        r'\\bbgmn\\b' : 'bagaimana',\n",
    "        r'\\bgimana\\b' : 'bagaimana',\n",
    "         r'\\bbyk\\b' : 'banyak',\n",
    "         r'\\bbodo amat\\b' : 'tidak peduli',\n",
    "         r'\\btuh\\b' : 'itu',\n",
    "         r'\\bngaca\\b' : 'intropeksi',\n",
    "        r'\\bmmbri\\b' : 'memberi'\n",
    "        \n",
    "    }\n",
    "    for pattern, replacement in slang_dict.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "    # HAPUS EMOJI & Simbol Aneh (Hanya sisakan huruf, angka, spasi, .,!?)\n",
    "    text = re.sub(r'[^a-z0-9\\s.,!?]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29d92a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>mohon ulama lurus dan k212 memberi hujjah part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>makanan beragam , harga makanan di food stall ...</td>\n",
       "      <td>makanan beragam , harga makanan di food stall ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pakai kartu kredit bca tidak untung malah rugi...</td>\n",
       "      <td>pakai kartu kredit bca tidak untung malah rugi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tempat unik , bagus buat foto , makanan enak ,...</td>\n",
       "      <td>tempat unik , bagus buat foto , makanan enak ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>saya bersama keluarga baru saja menikmati peng...</td>\n",
       "      <td>saya bersama keluarga baru saja menikmati peng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bersyukur</td>\n",
       "      <td>bersyukur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  warung ini dimiliki oleh pengusaha pabrik tahu...   \n",
       "1  mohon ulama lurus dan k212 mmbri hujjah partai...   \n",
       "2  lokasi strategis di jalan sumatera bandung . t...   \n",
       "3  betapa bahagia nya diri ini saat unboxing pake...   \n",
       "4  duh . jadi mahasiswa jangan sombong dong . kas...   \n",
       "5  makanan beragam , harga makanan di food stall ...   \n",
       "6  pakai kartu kredit bca tidak untung malah rugi...   \n",
       "7  tempat unik , bagus buat foto , makanan enak ,...   \n",
       "8  saya bersama keluarga baru saja menikmati peng...   \n",
       "9                                          bersyukur   \n",
       "\n",
       "                                          clean_text  \n",
       "0  warung ini dimiliki oleh pengusaha pabrik tahu...  \n",
       "1  mohon ulama lurus dan k212 memberi hujjah part...  \n",
       "2  lokasi strategis di jalan sumatera bandung . t...  \n",
       "3  betapa bahagia nya diri ini saat unboxing pake...  \n",
       "4  duh . jadi mahasiswa jangan sombong dong . kas...  \n",
       "5  makanan beragam , harga makanan di food stall ...  \n",
       "6  pakai kartu kredit bca tidak untung malah rugi...  \n",
       "7  tempat unik , bagus buat foto , makanan enak ,...  \n",
       "8  saya bersama keluarga baru saja menikmati peng...  \n",
       "9                                          bersyukur  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['clean_text'] = train['text'].apply(specialized_cleaning)\n",
    "train[['text', 'clean_text']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6aa3f65-0ab2-483f-9f6b-ba4ee839f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['clean_text'] = test['text'].apply(specialized_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f498aa-dfac-46fd-9b3e-21a1efc9a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'positive' : 2, 'negative' : 1 , 'neutral' : 0}\n",
    "\n",
    "train['label'] = train['sentiment'].map(label_dict)\n",
    "test['label'] = test['sentiment'].map(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa8be9e-abfe-45cc-86d6-52a03f49aec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mohon ulama lurus dan k212 memberi hujjah part...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>tidak kecewa</td>\n",
       "      <td>positive</td>\n",
       "      <td>tidak kecewa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>enak rasa masakan nya apalagi kepiting yang me...</td>\n",
       "      <td>positive</td>\n",
       "      <td>enak rasa masakan nya apalagi kepiting yang me...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>hormati partai-partai yang telah berkoalisi</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hormati partaipartai yang telah berkoalisi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>pagi pagi di tol pasteur sudah macet parah , b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>pagi pagi di tol pasteur sudah macet parah , b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>meskipun sering belanja ke yogya di riau junct...</td>\n",
       "      <td>positive</td>\n",
       "      <td>meskipun sering belanja ke yogya di riau junct...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "0      warung ini dimiliki oleh pengusaha pabrik tahu...  positive   \n",
       "1      mohon ulama lurus dan k212 mmbri hujjah partai...   neutral   \n",
       "2      lokasi strategis di jalan sumatera bandung . t...  positive   \n",
       "3      betapa bahagia nya diri ini saat unboxing pake...  positive   \n",
       "4      duh . jadi mahasiswa jangan sombong dong . kas...  negative   \n",
       "...                                                  ...       ...   \n",
       "10995                                       tidak kecewa  positive   \n",
       "10996  enak rasa masakan nya apalagi kepiting yang me...  positive   \n",
       "10997        hormati partai-partai yang telah berkoalisi   neutral   \n",
       "10998  pagi pagi di tol pasteur sudah macet parah , b...  negative   \n",
       "10999  meskipun sering belanja ke yogya di riau junct...  positive   \n",
       "\n",
       "                                              clean_text  label  \n",
       "0      warung ini dimiliki oleh pengusaha pabrik tahu...      2  \n",
       "1      mohon ulama lurus dan k212 memberi hujjah part...      0  \n",
       "2      lokasi strategis di jalan sumatera bandung . t...      2  \n",
       "3      betapa bahagia nya diri ini saat unboxing pake...      2  \n",
       "4      duh . jadi mahasiswa jangan sombong dong . kas...      1  \n",
       "...                                                  ...    ...  \n",
       "10995                                       tidak kecewa      2  \n",
       "10996  enak rasa masakan nya apalagi kepiting yang me...      2  \n",
       "10997         hormati partaipartai yang telah berkoalisi      0  \n",
       "10998  pagi pagi di tol pasteur sudah macet parah , b...      1  \n",
       "10999  meskipun sering belanja ke yogya di riau junct...      2  \n",
       "\n",
       "[11000 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d22fd00-b8bc-4459-a21e-c55d8454559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meski masa kampanye sudah selesai , bukan bera...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>meski masa kampanye sudah selesai , bukan bera...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tidak enak</td>\n",
       "      <td>negative</td>\n",
       "      <td>tidak enak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>restoran ini menawarkan makanan sunda . kami m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>restoran ini menawarkan makanan sunda . kami m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lokasi di alun alun masakan padang ini cukup t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>lokasi di alun alun masakan padang ini cukup t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>betapa bejad kader gerindra yang anggota dprd ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>betapa bejad kader gerindra yang anggota dprd ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>film tncfu , tidak cocok untuk penonton yang t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>film tncfu , tidak cocok untuk penonton yang t...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>indihome ini mahal loh bayar nya . hanya , pen...</td>\n",
       "      <td>negative</td>\n",
       "      <td>indihome ini mahal loh bayar nya . hanya , pen...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>be de gea , cowok cupu yang takut dengan pacar...</td>\n",
       "      <td>negative</td>\n",
       "      <td>be de gea , cowok cupu yang takut dengan pacar...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>valen yang sangat tidak berkualitas . konentat...</td>\n",
       "      <td>negative</td>\n",
       "      <td>valen yang sangat tidak berkualitas . konentat...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>restoran ini menjadi tempat pilihan saya berbu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>restoran ini menjadi tempat pilihan saya berbu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment  \\\n",
       "0     meski masa kampanye sudah selesai , bukan bera...   neutral   \n",
       "1                                            tidak enak  negative   \n",
       "2     restoran ini menawarkan makanan sunda . kami m...  positive   \n",
       "3     lokasi di alun alun masakan padang ini cukup t...  positive   \n",
       "4     betapa bejad kader gerindra yang anggota dprd ...  negative   \n",
       "...                                                 ...       ...   \n",
       "1255  film tncfu , tidak cocok untuk penonton yang t...  negative   \n",
       "1256  indihome ini mahal loh bayar nya . hanya , pen...  negative   \n",
       "1257  be de gea , cowok cupu yang takut dengan pacar...  negative   \n",
       "1258  valen yang sangat tidak berkualitas . konentat...  negative   \n",
       "1259  restoran ini menjadi tempat pilihan saya berbu...  positive   \n",
       "\n",
       "                                             clean_text  label  \n",
       "0     meski masa kampanye sudah selesai , bukan bera...      0  \n",
       "1                                            tidak enak     -1  \n",
       "2     restoran ini menawarkan makanan sunda . kami m...      1  \n",
       "3     lokasi di alun alun masakan padang ini cukup t...      1  \n",
       "4     betapa bejad kader gerindra yang anggota dprd ...     -1  \n",
       "...                                                 ...    ...  \n",
       "1255  film tncfu , tidak cocok untuk penonton yang t...     -1  \n",
       "1256  indihome ini mahal loh bayar nya . hanya , pen...     -1  \n",
       "1257  be de gea , cowok cupu yang takut dengan pacar...     -1  \n",
       "1258  valen yang sangat tidak berkualitas . konentat...     -1  \n",
       "1259  restoran ini menjadi tempat pilihan saya berbu...      1  \n",
       "\n",
       "[1260 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02ef96d-9b10-41e2-8757-0f5ecca27284",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('demo_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "264063a3-21fa-46c4-9cd4-18f1dd4cdc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['softeng.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump((train, test), 'softeng.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f9864a-4068-42c2-bacf-2b177b10b872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['softeng.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((train, test), 'softeng.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c862394",
   "metadata": {},
   "source": [
    "Preprocessing BERT masih simpan tanda baca, supaya menangkap konteks kalimat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57908284-44bf-4ec4-8be7-252e6a49d166",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e1a964-3304-49d0-ae6f-ee23f3cdb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "train, test = joblib.load('softeng.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5123c5f6-c5bf-4676-b9e1-460264145d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80991cf4-9d84-427f-960e-25d384103d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c2c8615-28c2-4fe2-a893-2f4106b03c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoTokenizer\t X_train\t joblib\t label_dict\t np\t pd\t re\t sns\t specialized_cleaning\t \n",
      "test\t tokenizer\t train\t y_train\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d1202cf-6af6-4532-9e32-d889e0989bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "132e2c5e-d497-4fbd-88ea-028444bf78ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d1a925-5bbb-4b8c-b6c7-ba1cbacc7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['clean_text']\n",
    "y_train = train['label']\n",
    "X_temp = test['clean_text']\n",
    "y_temp = test['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f40b20-f9a0-4967-a837-3a3df81e0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"answerdotai/ModernBERT-base\",\n",
    "    cache_dir=r'C:\\Users\\Abdullah Ghassan\\.cache\\huggingface\\hub'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aefa1cc6-a8c5-44ce-b0eb-d72a5fc6e2c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='answerdotai/ModernBERT-base', vocab_size=50280, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"|||IP_ADDRESS|||\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t1: AddedToken(\"<|padding|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50254: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50255: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50256: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50257: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50258: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50259: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50260: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50261: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50262: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50263: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50264: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50265: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50266: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50267: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50268: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50269: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50270: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50271: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50272: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50273: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50274: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50275: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50276: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50277: AddedToken(\"|||EMAIL_ADDRESS|||\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50278: AddedToken(\"|||PHONE_NUMBER|||\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50279: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50280: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50281: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50282: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50283: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50284: AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t50285: AddedToken(\"[unused0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50286: AddedToken(\"[unused1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50287: AddedToken(\"[unused2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50288: AddedToken(\"[unused3]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50289: AddedToken(\"[unused4]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50290: AddedToken(\"[unused5]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50291: AddedToken(\"[unused6]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50292: AddedToken(\"[unused7]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50293: AddedToken(\"[unused8]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50294: AddedToken(\"[unused9]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50295: AddedToken(\"[unused10]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50296: AddedToken(\"[unused11]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50297: AddedToken(\"[unused12]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50298: AddedToken(\"[unused13]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50299: AddedToken(\"[unused14]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50300: AddedToken(\"[unused15]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50301: AddedToken(\"[unused16]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50302: AddedToken(\"[unused17]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50303: AddedToken(\"[unused18]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50304: AddedToken(\"[unused19]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50305: AddedToken(\"[unused20]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50306: AddedToken(\"[unused21]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50307: AddedToken(\"[unused22]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50308: AddedToken(\"[unused23]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50309: AddedToken(\"[unused24]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50310: AddedToken(\"[unused25]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50311: AddedToken(\"[unused26]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50312: AddedToken(\"[unused27]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50313: AddedToken(\"[unused28]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50314: AddedToken(\"[unused29]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50315: AddedToken(\"[unused30]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50316: AddedToken(\"[unused31]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50317: AddedToken(\"[unused32]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50318: AddedToken(\"[unused33]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50319: AddedToken(\"[unused34]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50320: AddedToken(\"[unused35]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50321: AddedToken(\"[unused36]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50322: AddedToken(\"[unused37]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50323: AddedToken(\"[unused38]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50324: AddedToken(\"[unused39]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50325: AddedToken(\"[unused40]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50326: AddedToken(\"[unused41]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50327: AddedToken(\"[unused42]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50328: AddedToken(\"[unused43]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50329: AddedToken(\"[unused44]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50330: AddedToken(\"[unused45]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50331: AddedToken(\"[unused46]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50332: AddedToken(\"[unused47]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50333: AddedToken(\"[unused48]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50334: AddedToken(\"[unused49]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50335: AddedToken(\"[unused50]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50336: AddedToken(\"[unused51]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50337: AddedToken(\"[unused52]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50338: AddedToken(\"[unused53]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50339: AddedToken(\"[unused54]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50340: AddedToken(\"[unused55]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50341: AddedToken(\"[unused56]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50342: AddedToken(\"[unused57]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50343: AddedToken(\"[unused58]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50344: AddedToken(\"[unused59]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50345: AddedToken(\"[unused60]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50346: AddedToken(\"[unused61]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50347: AddedToken(\"[unused62]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50348: AddedToken(\"[unused63]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50349: AddedToken(\"[unused64]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50350: AddedToken(\"[unused65]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50351: AddedToken(\"[unused66]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50352: AddedToken(\"[unused67]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50353: AddedToken(\"[unused68]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50354: AddedToken(\"[unused69]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50355: AddedToken(\"[unused70]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50356: AddedToken(\"[unused71]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50357: AddedToken(\"[unused72]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50358: AddedToken(\"[unused73]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50359: AddedToken(\"[unused74]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50360: AddedToken(\"[unused75]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50361: AddedToken(\"[unused76]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50362: AddedToken(\"[unused77]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50363: AddedToken(\"[unused78]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50364: AddedToken(\"[unused79]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50365: AddedToken(\"[unused80]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50366: AddedToken(\"[unused81]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50367: AddedToken(\"[unused82]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b212ea5-f60d-41a3-927c-9183bea72e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "def create_dataset(texts, labels):\n",
    "    encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=36, return_tensors='pt')\n",
    "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels)) # would work here\n",
    "    return dataset\n",
    "\n",
    "# label must be convert to list so it worked for torch.tensor (reference line 5)\n",
    "y_1 = y_train.to_list()\n",
    "y_2 = y_val.to_list()\n",
    "y_3 = y_test.to_list()\n",
    "\n",
    "train_dataset = create_dataset(X_train, y_1)\n",
    "val_dataset   = create_dataset(X_val, y_2)\n",
    "test_dataset  = create_dataset(X_test, y_3)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_loader   = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7fe1e0a-c625-48a3-8c73-cd58ce66d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/modernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "model_id = \"answerdotai/modernBERT-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, cache_dir = \"C:/Users/Abdullah Ghassan/.cache/huggingface/hub\",\n",
    "                                                          num_labels = 3)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4585b495-8244-46b6-b8f0-902d322a805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "'''\n",
    "helper function for time and model metrics\n",
    "'''\n",
    "def format_time(elapsed):\n",
    "    return str(elapsed)\n",
    "\n",
    "def calculate_metrics(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    acc = accuracy_score(labels_flat, pred_flat)\n",
    "    f1 = f1_score(labels_flat, pred_flat, average='macro') \n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baffa67b-3d2f-4296-9b58-ff84ed721c26",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dimulai... Model terbaik akan disimpan di: best_model_modernBERT.pt\n",
      "\n",
      "======== Epoch 1 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:42<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 1:\n",
      "  Training Loss    : 0.6086\n",
      "  Validation Loss  : 0.5332\n",
      "  Validation Acc   : 0.7731\n",
      "  Validation F1    : 0.6566 (Macro)\n",
      "  [INFO] F1 Score meningkat (0.0000 --> 0.6566). Menyimpan model...\n",
      "\n",
      "======== Epoch 2 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:41<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 2:\n",
      "  Training Loss    : 0.3895\n",
      "  Validation Loss  : 0.5179\n",
      "  Validation Acc   : 0.8009\n",
      "  Validation F1    : 0.7053 (Macro)\n",
      "  [INFO] F1 Score meningkat (0.6566 --> 0.7053). Menyimpan model...\n",
      "\n",
      "======== Epoch 3 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:42<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 3:\n",
      "  Training Loss    : 0.2306\n",
      "  Validation Loss  : 0.4961\n",
      "  Validation Acc   : 0.8449\n",
      "  Validation F1    : 0.7542 (Macro)\n",
      "  [INFO] F1 Score meningkat (0.7053 --> 0.7542). Menyimpan model...\n",
      "\n",
      "======== Epoch 4 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:41<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 4:\n",
      "  Training Loss    : 0.1364\n",
      "  Validation Loss  : 0.8850\n",
      "  Validation Acc   : 0.8449\n",
      "  Validation F1    : 0.7607 (Macro)\n",
      "  [INFO] F1 Score meningkat (0.7542 --> 0.7607). Menyimpan model...\n",
      "\n",
      "======== Epoch 5 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:42<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 5:\n",
      "  Training Loss    : 0.0592\n",
      "  Validation Loss  : 1.2411\n",
      "  Validation Acc   : 0.8380\n",
      "  Validation F1    : 0.7484 (Macro)\n",
      "  [INFO] F1 Score tidak meningkat (Best: 0.7607).\n",
      "\n",
      "======== Epoch 6 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:42<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 6:\n",
      "  Training Loss    : 0.0203\n",
      "  Validation Loss  : 1.3176\n",
      "  Validation Acc   : 0.8310\n",
      "  Validation F1    : 0.7623 (Macro)\n",
      "  [INFO] F1 Score meningkat (0.7607 --> 0.7623). Menyimpan model...\n",
      "\n",
      "======== Epoch 7 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:44<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 7:\n",
      "  Training Loss    : 0.0162\n",
      "  Validation Loss  : 1.7001\n",
      "  Validation Acc   : 0.8032\n",
      "  Validation F1    : 0.6700 (Macro)\n",
      "  [INFO] F1 Score tidak meningkat (Best: 0.7623).\n",
      "\n",
      "======== Epoch 8 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:41<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 8:\n",
      "  Training Loss    : 0.0183\n",
      "  Validation Loss  : 1.6328\n",
      "  Validation Acc   : 0.8356\n",
      "  Validation F1    : 0.7455 (Macro)\n",
      "  [INFO] F1 Score tidak meningkat (Best: 0.7623).\n",
      "\n",
      "Training Selesai!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os # Untuk path file\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# PERSIAPAN PENYIMPANAN\n",
    "# ==========================================\n",
    "\n",
    "epochs = 8\n",
    "best_val_f1 = 0.0 # Rekor F1 tertinggi saat ini (inisialisasi 0)\n",
    "save_path = 'best_model_modernBERT.pt' # Nama file model\n",
    "\n",
    "print(f\"Training dimulai... Model terbaik akan disimpan di: {save_path}\")\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    # training core\n",
    "    print(f'\\n======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=None)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss = criterion(logits, b_labels)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss is NaN! Stopping training to debug.\")\n",
    "            # You can print other tensors here to inspect their values\n",
    "            # print(\"Logits:\", logits)\n",
    "            # print(\"Labels:\", b_labels)\n",
    "            break \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # --- FASE VALIDASI ---\n",
    "    print('\\nRunning Validation...')\n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_f1 = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=None)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, b_labels)\n",
    "            \n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        tmp_eval_accuracy, tmp_eval_f1 = calculate_metrics(logits, label_ids)\n",
    "        total_eval_accuracy += tmp_eval_accuracy\n",
    "        total_eval_f1 += tmp_eval_f1\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(val_loader)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
    "    avg_val_f1 = total_eval_f1 / len(val_loader)\n",
    "\n",
    "    print(f\"\\n  Hasil Epoch {epoch_i + 1}:\")\n",
    "    print(f\"  Training Loss    : {avg_train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss  : {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation Acc   : {avg_val_accuracy:.4f}\")\n",
    "    print(f\"  Validation F1    : {avg_val_f1:.4f} (Macro)\")\n",
    "\n",
    "    # ==========================================\n",
    "    # LOGIKA SIMPAN MODEL TERBAIK (CHECKPOINT)\n",
    "    # ==========================================\n",
    "    # Kita simpan jika F1 Score saat ini lebih besar dari rekor sebelumnya\n",
    "    if avg_val_f1 > best_val_f1:\n",
    "        print(f\"  [INFO] F1 Score meningkat ({best_val_f1:.4f} --> {avg_val_f1:.4f}). Menyimpan model...\")\n",
    "        \n",
    "        # Update rekor\n",
    "        best_val_f1 = avg_val_f1\n",
    "        \n",
    "        # Simpan state_dict (bobotnya saja, lebih hemat memori)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    else:\n",
    "        print(f\"  [INFO] F1 Score tidak meningkat (Best: {best_val_f1:.4f}).\")\n",
    "\n",
    "print(\"\\nTraining Selesai!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee48a22-0aff-4c89-b3cb-3085b730705d",
   "metadata": {},
   "source": [
    "# Handle Class Imbalance + Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44ecb889-e372-4d19-898f-9743f16293e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/modernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Hitung Class Weights (Penting untuk Imbalance!)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "model_id = \"answerdotai/modernBERT-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, cache_dir = \"C:/Users/Abdullah Ghassan/.cache/huggingface/hub\",\n",
    "                                                          num_labels = 3)\n",
    "model.to(device)\n",
    "state_dict = torch.load('best_model_modernBERT.pt', map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51e10594-dd62-4936-a1f0-3d814da46e41",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dimulai... Model terbaik akan disimpan di: best_model_modernBERT_2.pt\n",
      "\n",
      "======== Epoch 1 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:41<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 1:\n",
      "  Training Loss    : 0.0257\n",
      "  Validation Loss  : 3.0072\n",
      "  Validation Acc   : 0.8403\n",
      "  Validation F1    : 0.7622 (Macro)\n",
      "  [INFO] F1 Score meningkat (0.0000 --> 0.7622). Menyimpan model...\n",
      "\n",
      "======== Epoch 2 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:42<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 2:\n",
      "  Training Loss    : 0.0202\n",
      "  Validation Loss  : 2.8112\n",
      "  Validation Acc   : 0.8310\n",
      "  Validation F1    : 0.7145 (Macro)\n",
      "  [INFO] F1 Score tidak meningkat (Best: 0.7622).\n",
      "\n",
      "======== Epoch 3 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:41<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 3:\n",
      "  Training Loss    : 0.0327\n",
      "  Validation Loss  : 3.0040\n",
      "  Validation Acc   : 0.7986\n",
      "  Validation F1    : 0.7093 (Macro)\n",
      "  [INFO] F1 Score tidak meningkat (Best: 0.7622).\n",
      "\n",
      "======== Epoch 4 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:41<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 4:\n",
      "  Training Loss    : 0.0368\n",
      "  Validation Loss  : 3.1940\n",
      "  Validation Acc   : 0.8380\n",
      "  Validation F1    : 0.7555 (Macro)\n",
      "  [INFO] F1 Score tidak meningkat (Best: 0.7622).\n",
      "\n",
      "======== Epoch 5 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:41<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 5:\n",
      "  Training Loss    : 0.0153\n",
      "  Validation Loss  : 3.5489\n",
      "  Validation Acc   : 0.8380\n",
      "  Validation F1    : 0.7388 (Macro)\n",
      "  [INFO] F1 Score tidak meningkat (Best: 0.7622).\n",
      "\n",
      "======== Epoch 6 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:41<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 6:\n",
      "  Training Loss    : 0.0199\n",
      "  Validation Loss  : 3.6027\n",
      "  Validation Acc   : 0.8472\n",
      "  Validation F1    : 0.7456 (Macro)\n",
      "  [INFO] F1 Score tidak meningkat (Best: 0.7622).\n",
      "\n",
      "======== Epoch 7 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:41<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 7:\n",
      "  Training Loss    : 0.0347\n",
      "  Validation Loss  : 3.0344\n",
      "  Validation Acc   : 0.8449\n",
      "  Validation F1    : 0.7455 (Macro)\n",
      "  [INFO] F1 Score tidak meningkat (Best: 0.7622).\n",
      "\n",
      "======== Epoch 8 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 688/688 [02:41<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "\n",
      "  Hasil Epoch 8:\n",
      "  Training Loss    : 0.0249\n",
      "  Validation Loss  : 3.2360\n",
      "  Validation Acc   : 0.8264\n",
      "  Validation F1    : 0.7175 (Macro)\n",
      "  [INFO] F1 Score tidak meningkat (Best: 0.7622).\n",
      "\n",
      "Training Selesai!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os # Untuk path file\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# PERSIAPAN PENYIMPANAN\n",
    "# ==========================================\n",
    "\n",
    "epochs = 8\n",
    "best_val_f1 = 0\n",
    "save_path = 'best_model_modernBERT_2.pt' # Nama file model\n",
    "\n",
    "print(f\"Training dimulai... Model terbaik akan disimpan di: {save_path}\")\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    # training core\n",
    "    print(f'\\n======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=None)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss = criterion(logits, b_labels)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss is NaN! Stopping training to debug.\")\n",
    "            # You can print other tensors here to inspect their values\n",
    "            # print(\"Logits:\", logits)\n",
    "            # print(\"Labels:\", b_labels)\n",
    "            break \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # --- FASE VALIDASI ---\n",
    "    print('\\nRunning Validation...')\n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_f1 = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=None)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, b_labels)\n",
    "            \n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        tmp_eval_accuracy, tmp_eval_f1 = calculate_metrics(logits, label_ids)\n",
    "        total_eval_accuracy += tmp_eval_accuracy\n",
    "        total_eval_f1 += tmp_eval_f1\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(val_loader)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
    "    avg_val_f1 = total_eval_f1 / len(val_loader)\n",
    "\n",
    "    print(f\"\\n  Hasil Epoch {epoch_i + 1}:\")\n",
    "    print(f\"  Training Loss    : {avg_train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss  : {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation Acc   : {avg_val_accuracy:.4f}\")\n",
    "    print(f\"  Validation F1    : {avg_val_f1:.4f} (Macro)\")\n",
    "\n",
    "    # ==========================================\n",
    "    # LOGIKA SIMPAN MODEL TERBAIK (CHECKPOINT)\n",
    "    # ==========================================\n",
    "    # Kita simpan jika F1 Score saat ini lebih besar dari rekor sebelumnya\n",
    "    if avg_val_f1 > best_val_f1:\n",
    "        print(f\"  [INFO] F1 Score meningkat ({best_val_f1:.4f} --> {avg_val_f1:.4f}). Menyimpan model...\")\n",
    "        \n",
    "        # Update rekor\n",
    "        best_val_f1 = avg_val_f1\n",
    "        \n",
    "        # Simpan state_dict (bobotnya saja, lebih hemat memori)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    else:\n",
    "        print(f\"  [INFO] F1 Score tidak meningkat (Best: {best_val_f1:.4f}).\")\n",
    "\n",
    "print(\"\\nTraining Selesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34f2b0bf-5a2d-42f0-a579-c70cd01a1ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/modernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.74      0.66      0.70        87\n",
      "    Negative       0.82      0.75      0.78       263\n",
      "    Positive       0.88      0.93      0.90       490\n",
      "\n",
      "    accuracy                           0.85       840\n",
      "   macro avg       0.81      0.78      0.79       840\n",
      "weighted avg       0.84      0.85      0.84       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_id = \"answerdotai/modernBERT-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, cache_dir = \"C:/Users/Abdullah Ghassan/.cache/huggingface/hub\",\n",
    "                                                          num_labels = 3)\n",
    "model.load_state_dict(torch.load('best_model_modernBERT_2.pt'))\n",
    "model.to(device) \n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    predictions.extend(np.argmax(logits, axis=1).flatten())\n",
    "    true_labels.extend(label_ids.flatten())\n",
    "\n",
    "print(classification_report(true_labels, predictions, target_names=['Neutral', 'Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e16f3a5-a615-4cf1-9892-0934ac658202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meski masa kampanye sudah selesai , bukan bera...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>meski masa kampanye sudah selesai , bukan bera...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tidak enak</td>\n",
       "      <td>negative</td>\n",
       "      <td>tidak enak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>restoran ini menawarkan makanan sunda . kami m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>restoran ini menawarkan makanan sunda . kami m...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lokasi di alun alun masakan padang ini cukup t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>lokasi di alun alun masakan padang ini cukup t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>betapa bejad kader gerindra yang anggota dprd ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>betapa bejad kader gerindra yang anggota dprd ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>film tncfu , tidak cocok untuk penonton yang t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>film tncfu , tidak cocok untuk penonton yang t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>indihome ini mahal loh bayar nya . hanya , pen...</td>\n",
       "      <td>negative</td>\n",
       "      <td>indihome ini mahal loh bayar nya . hanya , pen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>be de gea , cowok cupu yang takut dengan pacar...</td>\n",
       "      <td>negative</td>\n",
       "      <td>be de gea , cowok cupu yang takut dengan pacar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>valen yang sangat tidak berkualitas . konentat...</td>\n",
       "      <td>negative</td>\n",
       "      <td>valen yang sangat tidak berkualitas . konentat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>restoran ini menjadi tempat pilihan saya berbu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>restoran ini menjadi tempat pilihan saya berbu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment  \\\n",
       "0     meski masa kampanye sudah selesai , bukan bera...   neutral   \n",
       "1                                            tidak enak  negative   \n",
       "2     restoran ini menawarkan makanan sunda . kami m...  positive   \n",
       "3     lokasi di alun alun masakan padang ini cukup t...  positive   \n",
       "4     betapa bejad kader gerindra yang anggota dprd ...  negative   \n",
       "...                                                 ...       ...   \n",
       "1255  film tncfu , tidak cocok untuk penonton yang t...  negative   \n",
       "1256  indihome ini mahal loh bayar nya . hanya , pen...  negative   \n",
       "1257  be de gea , cowok cupu yang takut dengan pacar...  negative   \n",
       "1258  valen yang sangat tidak berkualitas . konentat...  negative   \n",
       "1259  restoran ini menjadi tempat pilihan saya berbu...  positive   \n",
       "\n",
       "                                             clean_text  label  \n",
       "0     meski masa kampanye sudah selesai , bukan bera...      0  \n",
       "1                                            tidak enak      1  \n",
       "2     restoran ini menawarkan makanan sunda . kami m...      2  \n",
       "3     lokasi di alun alun masakan padang ini cukup t...      2  \n",
       "4     betapa bejad kader gerindra yang anggota dprd ...      1  \n",
       "...                                                 ...    ...  \n",
       "1255  film tncfu , tidak cocok untuk penonton yang t...      1  \n",
       "1256  indihome ini mahal loh bayar nya . hanya , pen...      1  \n",
       "1257  be de gea , cowok cupu yang takut dengan pacar...      1  \n",
       "1258  valen yang sangat tidak berkualitas . konentat...      1  \n",
       "1259  restoran ini menjadi tempat pilihan saya berbu...      2  \n",
       "\n",
       "[1260 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ccc4d-abb4-4380-8896-52562a5e2422",
   "metadata": {},
   "source": [
    "# Minta model prediski dengan data terurut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6de00450-6e61-4755-a184-fdb400dfacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = test['clean_text']\n",
    "y_temp = test['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=2/3, shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9aec438-b94c-468b-a910-40729613ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "def create_dataset(texts, labels):\n",
    "    encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=36, return_tensors='pt')\n",
    "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels)) # would work here\n",
    "    return dataset\n",
    "\n",
    "# label must be convert to list so it worked for torch.tensor (reference line 5)\n",
    "y_3 = y_test.to_list()\n",
    "test_dataset  = create_dataset(X_test, y_3)\n",
    "\n",
    "batch_size = 16\n",
    "test_loader  = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96a715d5-061e-4e06-b0e4-bfbc72e75959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/modernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.73      0.66      0.69        91\n",
      "    Negative       0.83      0.72      0.77       261\n",
      "    Positive       0.86      0.93      0.89       488\n",
      "\n",
      "    accuracy                           0.84       840\n",
      "   macro avg       0.81      0.77      0.79       840\n",
      "weighted avg       0.83      0.84      0.83       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_id = \"answerdotai/modernBERT-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, cache_dir = \"C:/Users/Abdullah Ghassan/.cache/huggingface/hub\",\n",
    "                                                          num_labels = 3)\n",
    "model.load_state_dict(torch.load('best_model_modernBERT_2.pt'))\n",
    "model.to(device) \n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    predictions.extend(np.argmax(logits, axis=1).flatten())\n",
    "    true_labels.extend(label_ids.flatten())\n",
    "\n",
    "print(classification_report(true_labels, predictions, target_names=['Neutral', 'Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b961ea05-1cee-4374-a216-1fbe3b3401d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['softeng.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((train, test, true_labels, predictions), 'softeng.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197c514-0e55-4f50-88ca-a34352b3e585",
   "metadata": {},
   "source": [
    "# Algoritma merge sample dengan hasil prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a4b89b8f-443b-4029-9410-bbd1a77dfb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediksi_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420</td>\n",
       "      <td>ustad bajingan seperti ustadtengkuzul sudah ka...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ustad bajingan seperti ustadtengkuzul sudah ka...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>421</td>\n",
       "      <td>lihatlah wahai netizen . apakah kalian masih p...</td>\n",
       "      <td>negative</td>\n",
       "      <td>lihatlah wahai netizen . apakah kalian masih p...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>422</td>\n",
       "      <td>siapkan payung jika ke sini siang hari . suasa...</td>\n",
       "      <td>positive</td>\n",
       "      <td>siapkan payung jika ke sini siang hari . suasa...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423</td>\n",
       "      <td>tribun : gubernur anies : kami ingin pasar sen...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>tribun gubernur anies kami ingin pasar senen j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>424</td>\n",
       "      <td>lailah banyak banget akun sampah model begini ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>lailah banyak banget akun sampah model begini ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>1255</td>\n",
       "      <td>film tncfu , tidak cocok untuk penonton yang t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>film tncfu , tidak cocok untuk penonton yang t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>1256</td>\n",
       "      <td>indihome ini mahal loh bayar nya . hanya , pen...</td>\n",
       "      <td>negative</td>\n",
       "      <td>indihome ini mahal loh bayar nya . hanya , pen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>1257</td>\n",
       "      <td>be de gea , cowok cupu yang takut dengan pacar...</td>\n",
       "      <td>negative</td>\n",
       "      <td>be de gea , cowok cupu yang takut dengan pacar...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>1258</td>\n",
       "      <td>valen yang sangat tidak berkualitas . konentat...</td>\n",
       "      <td>negative</td>\n",
       "      <td>valen yang sangat tidak berkualitas . konentat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>1259</td>\n",
       "      <td>restoran ini menjadi tempat pilihan saya berbu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>restoran ini menjadi tempat pilihan saya berbu...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>840 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               text sentiment  \\\n",
       "0      420  ustad bajingan seperti ustadtengkuzul sudah ka...  negative   \n",
       "1      421  lihatlah wahai netizen . apakah kalian masih p...  negative   \n",
       "2      422  siapkan payung jika ke sini siang hari . suasa...  positive   \n",
       "3      423  tribun : gubernur anies : kami ingin pasar sen...   neutral   \n",
       "4      424  lailah banyak banget akun sampah model begini ...  negative   \n",
       "..     ...                                                ...       ...   \n",
       "835   1255  film tncfu , tidak cocok untuk penonton yang t...  negative   \n",
       "836   1256  indihome ini mahal loh bayar nya . hanya , pen...  negative   \n",
       "837   1257  be de gea , cowok cupu yang takut dengan pacar...  negative   \n",
       "838   1258  valen yang sangat tidak berkualitas . konentat...  negative   \n",
       "839   1259  restoran ini menjadi tempat pilihan saya berbu...  positive   \n",
       "\n",
       "                                            clean_text  label  prediksi_model  \n",
       "0    ustad bajingan seperti ustadtengkuzul sudah ka...      1               1  \n",
       "1    lihatlah wahai netizen . apakah kalian masih p...      1               1  \n",
       "2    siapkan payung jika ke sini siang hari . suasa...      2               2  \n",
       "3    tribun gubernur anies kami ingin pasar senen j...      0               0  \n",
       "4    lailah banyak banget akun sampah model begini ...      1               2  \n",
       "..                                                 ...    ...             ...  \n",
       "835  film tncfu , tidak cocok untuk penonton yang t...      1               1  \n",
       "836  indihome ini mahal loh bayar nya . hanya , pen...      1               1  \n",
       "837  be de gea , cowok cupu yang takut dengan pacar...      1               1  \n",
       "838  valen yang sangat tidak berkualitas . konentat...      1               1  \n",
       "839  restoran ini menjadi tempat pilihan saya berbu...      2               2  \n",
       "\n",
       "[840 rows x 6 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "goals: merge sample input buat model prediksi (untuk kasus ini contohnya sample input = test dataframe) dengan hasil prediksi\n",
    "\n",
    "slicing logic -> test.iloc[1259-839:] (slicing 839 baris terakhir dari dataframe) [1]\n",
    "code row_start untuk df_temp = iloc dibawah berdasarkan logic dari referensi [1] supaya bisa slicing secara dinamis, berapapun panjang dari sample input\n",
    "\n",
    "flow: slicing ->  reset index sample dan hasil prediksi -> merge\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "row_start = test.shape[0] - len(predictions) \n",
    "df_temp = test.iloc[row_start:]\n",
    "df_temp = df_temp.reset_index()\n",
    "pred_temp = pd.Series(predictions, name = 'prediksi_model')\n",
    "result2 = pd.merge(df_temp, pred_temp, left_index = True, right_index = True, how = 'left')\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2924f436-2162-410d-a8bf-32cff85c68d2",
   "metadata": {},
   "source": [
    "# Algoritma ambil data dengan sentiment yang negative saja\n",
    "\n",
    "## data dengan sentiment yang negative ini akan dijadikan input ke topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "282c7455-4f83-4600-b4f9-35fb28da572c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediksi_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420</td>\n",
       "      <td>ustad bajingan seperti ustadtengkuzul sudah ka...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ustad bajingan seperti ustadtengkuzul sudah ka...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>421</td>\n",
       "      <td>lihatlah wahai netizen . apakah kalian masih p...</td>\n",
       "      <td>negative</td>\n",
       "      <td>lihatlah wahai netizen . apakah kalian masih p...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>430</td>\n",
       "      <td>kayak nya mendingan gocar deh</td>\n",
       "      <td>positive</td>\n",
       "      <td>kayak nya mendingan gocar deh</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>431</td>\n",
       "      <td>tidak ganteng</td>\n",
       "      <td>negative</td>\n",
       "      <td>tidak ganteng</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>433</td>\n",
       "      <td>bandung macet banget ada apa si</td>\n",
       "      <td>negative</td>\n",
       "      <td>bandung macet banget ada apa si</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>1253</td>\n",
       "      <td>gila saja sih nih handphone samsung mahal bang...</td>\n",
       "      <td>negative</td>\n",
       "      <td>gila saja sih nih handphone samsung mahal bang...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>1255</td>\n",
       "      <td>film tncfu , tidak cocok untuk penonton yang t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>film tncfu , tidak cocok untuk penonton yang t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>1256</td>\n",
       "      <td>indihome ini mahal loh bayar nya . hanya , pen...</td>\n",
       "      <td>negative</td>\n",
       "      <td>indihome ini mahal loh bayar nya . hanya , pen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>1257</td>\n",
       "      <td>be de gea , cowok cupu yang takut dengan pacar...</td>\n",
       "      <td>negative</td>\n",
       "      <td>be de gea , cowok cupu yang takut dengan pacar...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>1258</td>\n",
       "      <td>valen yang sangat tidak berkualitas . konentat...</td>\n",
       "      <td>negative</td>\n",
       "      <td>valen yang sangat tidak berkualitas . konentat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>229 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               text sentiment  \\\n",
       "0      420  ustad bajingan seperti ustadtengkuzul sudah ka...  negative   \n",
       "1      421  lihatlah wahai netizen . apakah kalian masih p...  negative   \n",
       "10     430                      kayak nya mendingan gocar deh  positive   \n",
       "11     431                                      tidak ganteng  negative   \n",
       "13     433                    bandung macet banget ada apa si  negative   \n",
       "..     ...                                                ...       ...   \n",
       "833   1253  gila saja sih nih handphone samsung mahal bang...  negative   \n",
       "835   1255  film tncfu , tidak cocok untuk penonton yang t...  negative   \n",
       "836   1256  indihome ini mahal loh bayar nya . hanya , pen...  negative   \n",
       "837   1257  be de gea , cowok cupu yang takut dengan pacar...  negative   \n",
       "838   1258  valen yang sangat tidak berkualitas . konentat...  negative   \n",
       "\n",
       "                                            clean_text  label  prediksi_model  \n",
       "0    ustad bajingan seperti ustadtengkuzul sudah ka...      1               1  \n",
       "1    lihatlah wahai netizen . apakah kalian masih p...      1               1  \n",
       "10                       kayak nya mendingan gocar deh      2               1  \n",
       "11                                       tidak ganteng      1               1  \n",
       "13                     bandung macet banget ada apa si      1               1  \n",
       "..                                                 ...    ...             ...  \n",
       "833  gila saja sih nih handphone samsung mahal bang...      1               1  \n",
       "835  film tncfu , tidak cocok untuk penonton yang t...      1               1  \n",
       "836  indihome ini mahal loh bayar nya . hanya , pen...      1               1  \n",
       "837  be de gea , cowok cupu yang takut dengan pacar...      1               1  \n",
       "838  valen yang sangat tidak berkualitas . konentat...      1               1  \n",
       "\n",
       "[229 rows x 6 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_modelling_input = result2.loc[result2['prediksi_model'] == 1]\n",
    "topic_modelling_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77676781-975a-4d41-95fd-79ad1635159c",
   "metadata": {},
   "source": [
    "# Topic Modelling manual (SBERT -> UMAP -> HDBSCAN -> c-TFIDF -> labelling machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d87b84fd-9a9c-48d5-b4e0-83cede51d8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['softeng.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((train, test, true_labels, predictions, topic_modelling_input, sbert_input, sbert_output, umap_output, hdb_machine, topics, probs, topic_modelling_output, topic_info), 'softeng.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa38843b-f5fd-4072-a005-332ec9b5e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "train, test, true_labels, predictions, topic_modelling_input, sbert_input, sbert_output, umap_output, hdb_machine, topics, probs, topic_modelling_output, topic_info = joblib.load('softeng.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b09a5cf8-a14e-4666-bdd6-b223ee45ce6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backend.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((topics, probs), 'backend.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f532c63-2e11-4a39-9d49-a1025e66d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('naufalihsan/indonesian-sbert-large')\n",
    "\n",
    "sbert_input = topic_modelling_input['clean_text'].to_list()\n",
    "len(sbert_input)\n",
    "\n",
    "sbert_output = model.encode(sbert_input)\n",
    "sbert_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f95e74-6179-468f-9818-8187c4d1d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(n_components = 5)\n",
    "umap_output = reducer.fit_transform(sbert_output)\n",
    "\n",
    "umap_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c259a-869f-4245-91b1-32dd90529615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN\n",
    "from sklearn.cluster import HDBSCAN\n",
    "hdb_machine = HDBSCAN(copy = True, max_cluster_size = 6)\n",
    "hdb_machine.fit(umap_output)\n",
    "\n",
    "hdb_machine.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19f71b8c-744b-47c7-96cc-dfc9bac1906a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1,  1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1,\n",
       "       -1, -1, -1,  2, -1, -1, -1, -1,  2, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1,  2, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1,  1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  2, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  2, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1,  1], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdb_machine.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203410b2-8faa-40b5-a189-b6439a67cbe0",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739f3567-84d3-42b9-9eec-5aeec60dad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(embedding_model = 'naufalihsan/indonesian-sbert-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ca8c43-1566-4bf4-8f16-ef5dd485a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = topic_modelling_input['clean_text']\n",
    "topics, probs = topic_model.fit_transform(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7fb00ff-c051-4e2e-98f4-3a783a35ce6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 15:09:49,794 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "topic_model.save(\"topic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "856fbf0c-6bba-4471-9a2c-5a6ed93fafe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>17</td>\n",
       "      <td>-1_bosan_sinetron_banget_jalan</td>\n",
       "      <td>[bosan, sinetron, banget, jalan, orang, gopay,...</td>\n",
       "      <td>[kenapa ya kalau nyepi mesti matikan lampu ?! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0_nya_itu_yang_ini</td>\n",
       "      <td>[nya, itu, yang, ini, tidak, di, saja, jokowi,...</td>\n",
       "      <td>[ya maka dari itu , yang dibilang dari tadi ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>1_tidak_saya_nya_indosat</td>\n",
       "      <td>[tidak, saya, nya, indosat, marah, di, ya, yan...</td>\n",
       "      <td>[halo minimal , kok tidak ditanggal ya , saya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>2_nya_di_makan_dan</td>\n",
       "      <td>[nya, di, makan, dan, yang, tidak, dengan, mak...</td>\n",
       "      <td>[tempat nya sungguh bagus , suasa nya juga ker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>3_melelahkan_tidur_mendingan_menyenangkan</td>\n",
       "      <td>[melelahkan, tidur, mendingan, menyenangkan, m...</td>\n",
       "      <td>[hari minggu ya panjang dan melelahkan, senin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                       Name  \\\n",
       "0     -1     17             -1_bosan_sinetron_banget_jalan   \n",
       "1      0    115                         0_nya_itu_yang_ini   \n",
       "2      1     54                   1_tidak_saya_nya_indosat   \n",
       "3      2     30                         2_nya_di_makan_dan   \n",
       "4      3     13  3_melelahkan_tidur_mendingan_menyenangkan   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [bosan, sinetron, banget, jalan, orang, gopay,...   \n",
       "1  [nya, itu, yang, ini, tidak, di, saja, jokowi,...   \n",
       "2  [tidak, saya, nya, indosat, marah, di, ya, yan...   \n",
       "3  [nya, di, makan, dan, yang, tidak, dengan, mak...   \n",
       "4  [melelahkan, tidur, mendingan, menyenangkan, m...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [kenapa ya kalau nyepi mesti matikan lampu ?! ...  \n",
       "1  [ya maka dari itu , yang dibilang dari tadi ju...  \n",
       "2  [halo minimal , kok tidak ditanggal ya , saya ...  \n",
       "3  [tempat nya sungguh bagus , suasa nya juga ker...  \n",
       "4  [hari minggu ya panjang dan melelahkan, senin ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f26aaf80-7b62-4cf4-9cf8-4f298944cadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nya', 0.05200474852846109),\n",
       " ('itu', 0.04833900169633662),\n",
       " ('yang', 0.0482272364278341),\n",
       " ('ini', 0.04551919864051053),\n",
       " ('tidak', 0.04439198116743297),\n",
       " ('di', 0.03794677282661403),\n",
       " ('saja', 0.03724966002506158),\n",
       " ('jokowi', 0.03413401432289205),\n",
       " ('sudah', 0.03394987692344164),\n",
       " ('dan', 0.03008658229862419)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10bd18c8-eb0c-4806-be62-b85453aeda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modelling_output = topic_modelling_input.copy()\n",
    "topic_modelling_output['topics'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "991a7214-4e18-4a00-8bf0-8dc255bc875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modelling_output.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4d632-dc4a-4a02-8822-b84d27970875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic.load('topic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6e33fe-521b-4359-9486-090feb8f0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract top words per topic\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_descriptions = {}\n",
    "for topic_id in topic_info['Topic']:\n",
    "    if topic_id == -1: \n",
    "        continue\n",
    "    words = [word for word, _ in topic_model.get_topic(topic_id)]\n",
    "    topic_descriptions[topic_id] = \", \".join(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166d3425-4a86-4df1-ade0-4176b9585a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6f759473864ef4852ec3d862610958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Use LLaMA to generate better labels or summaries\n",
    "# Load a local LLaMA (e.g., via Hugging Face if you have access)\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/Phi-3-mini-4k-instruct\",  # or use \"unsloth/Llama-3.1-8B-Instruct\" for efficient inference\n",
    "    device='cuda',\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7229f01d-ee04-4d05-989f-84e078c2db49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                             Name                CustomName  \\\n",
      "0     -1     26         -1_nya_bosan_kalau_orang  -1_nya_bosan_kalau_orang   \n",
      "1      0    106               0_itu_nya_yang_ini           Jokowi's Policy   \n",
      "2      1     55       1_tidak_saya_indosat_marah             Indosat Anger   \n",
      "3      2     30               2_nya_di_makan_dan     \"Kami Makanan Tidak D   \n",
      "4      3     12  3_melelahkan_kagum_titik_minggu  Efisien Perjalanan Kelas   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [nya, bosan, kalau, orang, sinetron, saja, yan...   \n",
      "1  [itu, nya, yang, ini, tidak, di, jokowi, saja,...   \n",
      "2  [tidak, saya, indosat, marah, nya, di, yang, a...   \n",
      "3  [nya, di, makan, dan, yang, tidak, dengan, mak...   \n",
      "4  [melelahkan, kagum, titik, minggu, killer, kel...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [feeling gue karena yang dibahas itu gopay , k...  \n",
      "1  [ya maka dari itu , yang dibilang dari tadi ju...  \n",
      "2  [halo minimal , kok tidak ditanggal ya , saya ...  \n",
      "3  [tempat nya sungguh bagus , suasa nya juga ker...  \n",
      "4  [hari minggu ya panjang dan melelahkan, titik ...  \n"
     ]
    }
   ],
   "source": [
    "def refine_topic_label(top_words: str) -> str:\n",
    "    prompt = f\"\"\"Based on the following key terms, give a short, clear, and general topic name (2-4 words max):\n",
    "    \n",
    "    Key terms: {top_words}\n",
    "    \n",
    "    Topic name:\"\"\"\n",
    "    \n",
    "    response = llm(prompt, max_new_tokens=10, do_sample=False)\n",
    "    # Extract generated text (adjust based on tokenizer/chat template)\n",
    "    generated = response[0]['generated_text'].split(\"Topic name:\")[-1].strip()\n",
    "    return generated.split(\"\\n\")[0].strip()\n",
    "\n",
    "# Apply to each topic\n",
    "refined_labels = {}\n",
    "for topic_id, words in topic_descriptions.items():\n",
    "    refined_labels[topic_id] = refine_topic_label(words)\n",
    "\n",
    "# Update BERTopic\n",
    "topic_model.set_topic_labels(refined_labels)\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0071d72f-8383-4b30-897a-3501611cb1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: \"Jokowi's Policy\",\n",
       " 1: 'Indosat Anger',\n",
       " 2: '\"Kami Makanan Tidak D',\n",
       " 3: 'Efisien Perjalanan Kelas'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eda08949-68e6-480c-90c3-5d91008a18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_labels = {0: \"Jokowi's Policy\",\n",
    " 1: 'Indosat Anger',\n",
    " 2: '\"Kami Makanan Tidak D',\n",
    " 3: 'Efisien Perjalanan Kelas'}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
